{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0           1                                                  5\n",
       "0        0  1467810672  is upset that he can't update his Facebook by ...\n",
       "1        0  1467810917  @Kenichan I dived many times for the ball. Man...\n",
       "2        0  1467811184    my whole body feels itchy and like its on fire \n",
       "3        0  1467811193  @nationwideclass no, it's not behaving at all....\n",
       "4        0  1467811372                      @Kwesidei not the whole crew \n",
       "...     ..         ...                                                ...\n",
       "1599994  4  2193601966  Just woke up. Having no school is the best fee...\n",
       "1599995  4  2193601969  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599996  4  2193601991  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599997  4  2193602064  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599998  4  2193602129  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1599999 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = dataframe.drop(columns=[2, 3, 4])\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.columns = [\"sentiment\", \"id\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>388956</th>\n",
       "      <td>0</td>\n",
       "      <td>2054349109</td>\n",
       "      <td>Open laps of race car driving school - I am 3r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162062</th>\n",
       "      <td>4</td>\n",
       "      <td>1979559733</td>\n",
       "      <td>@DwightHoward Represent the East! Beat the Lak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511620</th>\n",
       "      <td>4</td>\n",
       "      <td>2175119636</td>\n",
       "      <td>@_anoushka_ But that's the whole point of Twit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311957</th>\n",
       "      <td>0</td>\n",
       "      <td>2001414232</td>\n",
       "      <td>i miss those moments when all i ever thought o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155487</th>\n",
       "      <td>4</td>\n",
       "      <td>1979015630</td>\n",
       "      <td>@davetran :O   where did he get his drivers li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283220</th>\n",
       "      <td>4</td>\n",
       "      <td>2001917395</td>\n",
       "      <td>@evliving  OMG...you are so funny...I've said ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47693</th>\n",
       "      <td>0</td>\n",
       "      <td>1677694405</td>\n",
       "      <td>@Tygatyga uh, i think you should have been pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337788</th>\n",
       "      <td>4</td>\n",
       "      <td>2017818673</td>\n",
       "      <td>Morning pumpkin doodles  hope everyone has a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569888</th>\n",
       "      <td>4</td>\n",
       "      <td>2188487053</td>\n",
       "      <td>@imogenheap I have to say, tonight, I am lovin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195171</th>\n",
       "      <td>4</td>\n",
       "      <td>1984581766</td>\n",
       "      <td>still sleepy. i was up at 5 am. anyhoo, good m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id  \\\n",
       "388956           0  2054349109   \n",
       "1162062          4  1979559733   \n",
       "1511620          4  2175119636   \n",
       "311957           0  2001414232   \n",
       "1155487          4  1979015630   \n",
       "...            ...         ...   \n",
       "1283220          4  2001917395   \n",
       "47693            0  1677694405   \n",
       "1337788          4  2017818673   \n",
       "1569888          4  2188487053   \n",
       "1195171          4  1984581766   \n",
       "\n",
       "                                                      text  \n",
       "388956   Open laps of race car driving school - I am 3r...  \n",
       "1162062  @DwightHoward Represent the East! Beat the Lak...  \n",
       "1511620  @_anoushka_ But that's the whole point of Twit...  \n",
       "311957   i miss those moments when all i ever thought o...  \n",
       "1155487  @davetran :O   where did he get his drivers li...  \n",
       "...                                                    ...  \n",
       "1283220  @evliving  OMG...you are so funny...I've said ...  \n",
       "47693    @Tygatyga uh, i think you should have been pla...  \n",
       "1337788  Morning pumpkin doodles  hope everyone has a s...  \n",
       "1569888  @imogenheap I have to say, tonight, I am lovin...  \n",
       "1195171  still sleepy. i was up at 5 am. anyhoo, good m...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = dataframe.sample(n=10000)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    5015\n",
       "0    4985\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1979559733,\n",
       "       '@DwightHoward Represent the East! Beat the Lakers!!! '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 4, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = np.array([x[0] for x in dataframe])\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {0: 0, 4: 1}\n",
    "labels = [label_map[label] for label in sentiment]\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = [tokenizer(x[2], return_tensors='pt', padding='max_length', max_length=128) for x in dataframe]\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(embedding, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = torch.stack([item['input_ids'][0] for item in X_train])\n",
    "attention_mask_train  = torch.stack([item['attention_mask'][0] for item in X_train])\n",
    "token_type_ids_train  = torch.stack([item['token_type_ids'][0] for item in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_test = torch.stack([item['input_ids'][0] for item in X_test])\n",
    "attention_mask_test  = torch.stack([item['attention_mask'][0] for item in X_test])\n",
    "token_type_ids_test  = torch.stack([item['token_type_ids'][0] for item in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training bert for baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids_train, attention_mask_train, y_train)\n",
    "train_dataloader = DataLoader(dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(bert_model.parameters(), lr=.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7298\n",
      "Epoch 2/5, Loss: 0.7082\n",
      "Epoch 3/5, Loss: 0.7088\n",
      "Epoch 4/5, Loss: 0.7085\n",
      "Epoch 5/5, Loss: 0.7059\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "bert_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for input_ids, attention_mask, label in train_dataloader:\n",
    "\n",
    "        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask, labels=label)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(input_ids_test, attention_mask_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.5075\n",
      "F1 Score: 0.3417\n"
     ]
    }
   ],
   "source": [
    "bert_model.eval()\n",
    "total_eval_accuracy = 0\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, label in test_dataloader:\n",
    "        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        accuracy = (preds == label).float().mean()\n",
    "        \n",
    "        total_eval_accuracy += accuracy.item()\n",
    "        predictions.extend(preds.numpy())\n",
    "        true_labels.extend(label.numpy())\n",
    "\n",
    "\n",
    "avg_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert with cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8192, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "cnn_model = CNN(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done\n",
      "Epoch 1/5, Loss: 0.6955\n",
      "epoch done\n",
      "Epoch 2/5, Loss: 0.6932\n",
      "epoch done\n",
      "Epoch 3/5, Loss: 0.6932\n",
      "epoch done\n",
      "Epoch 4/5, Loss: 0.6932\n",
      "epoch done\n",
      "Epoch 5/5, Loss: 0.6932\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "cnn_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for input_ids, attention_mask, label in train_dataloader:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bert_outputs = bert_model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            bert_embeddings = bert_outputs.last_hidden_state\n",
    "            bert_embeddings = bert_embeddings.permute(0, 2, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = cnn_model(bert_embeddings)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(\"epoch done\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.4925\n",
      "F1 Score: 0.3250\n"
     ]
    }
   ],
   "source": [
    "total_eval_accuracy = 0\n",
    "cnn_model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, label in test_dataloader:\n",
    "        bert_outputs = bert_model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_embeddings = bert_outputs.last_hidden_state\n",
    "        bert_embeddings = bert_embeddings.permute(0, 2, 1)\n",
    "        cnn_logits = cnn_model(bert_embeddings)\n",
    "\n",
    "        preds = torch.argmax(cnn_logits, dim=1)\n",
    "        \n",
    "        accuracy = (preds == label).float().mean()        \n",
    "        total_eval_accuracy += accuracy.item()\n",
    "\n",
    "        predictions.extend(preds.numpy())\n",
    "        true_labels.extend(label.numpy())\n",
    "\n",
    "avg_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert with fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FCN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "fcn_model = FCN(98304, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done\n",
      "Epoch 1/5, Loss: 0.6932\n",
      "epoch done\n",
      "Epoch 2/5, Loss: 0.6932\n",
      "epoch done\n",
      "Epoch 3/5, Loss: 0.6932\n",
      "epoch done\n",
      "Epoch 4/5, Loss: 0.6932\n",
      "epoch done\n",
      "Epoch 5/5, Loss: 0.6932\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "cnn_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for input_ids, attention_mask, label in train_dataloader:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bert_outputs = bert_model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            bert_embeddings = bert_outputs.last_hidden_state\n",
    "            bert_embeddings = bert_embeddings.permute(0, 2, 1).reshape(bert_embeddings.size(0), -1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = fcn_model(bert_embeddings)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(\"epoch done\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.5075\n",
      "F1 Score: 0.3417\n"
     ]
    }
   ],
   "source": [
    "total_eval_accuracy = 0\n",
    "fcn_model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, label in test_dataloader:\n",
    "        bert_outputs = bert_model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_embeddings = bert_outputs.last_hidden_state\n",
    "        \n",
    "        bert_embeddings = bert_embeddings.view(bert_embeddings.size(0), -1)\n",
    "        \n",
    "        fcn_logits = fcn_model(bert_embeddings)\n",
    "\n",
    "        preds = torch.argmax(fcn_logits, dim=1)\n",
    "        \n",
    "        accuracy = (preds == label).float().mean()        \n",
    "        total_eval_accuracy += accuracy.item()\n",
    "\n",
    "        predictions.extend(preds.numpy())\n",
    "        true_labels.extend(label.numpy())\n",
    "\n",
    "\n",
    "avg_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
